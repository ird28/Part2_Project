% Isaac Dunn Part II Computer Science Dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage[T1]{fontenc}
\usepackage{mathpazo}  % style
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\graphicspath{ {./figs/} }
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{enumitem}
\usepackage{color}
\usepackage{listings}
\usepackage{subcaption}
\usepackage[labelfont=bf]{caption}

\captionsetup[algorithm]{labelformat=empty}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{grey}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{ % style of code listings
	frame=tb,
	language=ML,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{grey},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=4
	}


\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

% "let a = b in" for use in algorithms
\newcommand{\Let}[2]{\State \textbf{let} #1 = #2 \textbf{in}}

% For use in the happens-before relation varient
\newcommand{\longhookrightarrow}
	{\ensuremath{\lhook\joinrel\longrightarrow}}


\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title page


\pagestyle{empty}

\rightline{\LARGE \textbf{Isaac Dunn}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Dynamic Partial-Order Reduction for Model Checking} \\[7mm]
Computer Science Tripos -- Part II \\[6mm]
Clare College \\[7mm]
\LARGE \today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:           & \bf Isaac Dunn                            			 \\
College:        & \bf Clare College                    				     \\
Project Title:	& \bf Dynamic Partial-Order Reduction for Model Checking \\
Examination:    & \bf Computer Science Tripos -- Part II, June 2016      \\
Word Count:     & \bf N    \\
Supervisors:	& \bf Dr. Jonathan Hayman \& Prof. Glynn Winskel             \\ 
\end{tabular}
}


\section*{Original Aim of the Project}

The original aim of the project was to build a system which
used dynamic partial-order reduction to improve the performance
of a model checking algorithm which could be used to find
assertion violations and deadlocks.

\section*{Work Completed}

The original aim of the project was met and exceeded, with
a stateful implementation of dynamic partial-order reduction
also being produced.

\section*{Special Difficulties}

No special difficulties were encountered, only the usual dozens of
difficult-to-find bugs.
 
\newpage
\section*{Declaration}

I, Isaac Dunn of Clare College, being a candidate for Part II of the Computer
Science Tripos, hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed:}

\bigskip
\leftline{Date:}

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

Put acknowledgements here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the body

\pagestyle{headings}

\chapter{Introduction}
\textbf{This project implements DPOR. This
	chapter motivates it.}

\section{Model Checking}
\textbf{Motivate the need for model checking}

We want to be very sure that some computer
programs are correct. Testing is not good
enough -- all software products have been
tested, but according to Dijkstra,
\emph{``Testing shows the presence,
	not the absence, of bugs''}.

Model checking is a category of formal
methods, in which \textbf{(basic explanation of
model checking)}

\section{The State Explosion Problem}
\textbf{Explain the motivation for techniques
	to reduce the size of the state space explored}

Concurrent software programs in practice often
use threads as their model. The number of possible
interleavings of concurrent threads blows up very 
quickly.

Give some maths, some empirical results,
and a state space diagram showing this.

\section{Partial-Order Reduction}
\textbf{Introduce POR as a technique that
	addresses the state explosion problem}

Idea: some interleavings are equivalent. If you
can identifying which are equivalent, you only
have to explore a subset. \emph{Give picture.}

Partial-order reduction is a category of techniques
that aim to identify such equivalences and so
perform model checking more efficiently.

\section{Dynamic Partial-Order Reduction}
\textbf{Introduce DPOR as a particular
	POR algorithm.}

DPOR uses runtime information to identify
equivalent interleavings. In fact, it assumes
that all interleavings are equivalent, picks
one arbitrarily, and takes note of backtracking
points which might lead to un-equivalent
interleavings, which are then explored on
backtrack.

\section{This Project}
\textbf{Explain aims of project, and why I
	chose it.}

The aim of this project is to implement DPOR.

It succeeded, and went further, implementing
a stateful version of DPOR, in which a table
of visited states is kept, and when a state is
re-visited, it is not re-explored. However, care
must be taken to make sure that this approach is
sound.

I chose this project to see how theory is
implemented in practice.

\chapter{Preparation}
The main preparation that was necessary before
the implementation of the project was the
development of a firm understanding of an
area of computer science that was new to me,
and in particular an understanding of the
DPOR algorithm itself. This was achieved
by study of the literature -- I began with
the paper introducing DPOR, and mostly
recursively followed references.

This chapter gives an overview of
the theoretical background, states precisely
what I mean by model checking, and briefly
introduces the DPOR algorithm.

\section{Background Definitions} \label{sec:background-defs}
\textbf{This section introduces the mathematical
	structures we are considering and operating on
	in this project.
	Not sure how to do this -- currently too
	abstract and unfriendly to read?}

\subsection{Processes and States}
We consider a system consisting of a finite set, $\mathcal{P}$,
of concurrent threads or processes (I use ``thread" and
``process" interchangeably throughout).
Each thread has its own local state, $s \in \mathcal{S}$, and there
is some shared global state, $g \in \mathcal{G}$. The overall
state of the system at any instant is therefore a member of the set
$ \textit{State} = (\mathcal{P} \to \mathcal{S}) \times \mathcal{G} $.

Each process executes a sequence of operations, as
specified in some programming language, each of which can
operate on the thread's local state $s$ or the shared
state $g$. If an operation
operates on $g$, it is said to be \emph{visible}, else it is said to be
\emph{invisible}.

\subsection{Transitions}
A \emph{transition} moves the system from one state to another,
by performing a finite sequence of invisible operations of some
process $p$, followed by a visible operation of the same process.
If process $p$ has local state $s$, then the transition $t_{p,s}$
that $p$ can make is defined to be a partial function taking the current
global state $g$ and giving $(s', g')$, the next local state for $p$
and the next global state for the system. Let $\mathcal{T}$ denote the
set of all transitions, so that
	\[\mathcal{T} = \mathcal{G} \rightharpoonup
				(\mathcal{S} \times \mathcal{G}).\]

A transition $t_{p,s}$ is said to be \emph{enabled} in a state
$(l, g)$ if and only if $l(p) = s$ and $t_{p,s}(g)$ is defined.
If $t_{p,s}$ is enabled in $\sigma = (l, g)$ and 
$t_{p,s}(g) = (s', g')$, then we say that the
execution of $t_{p,s}$ from $\sigma$ results in the unique successor
state $\sigma' = (l', g')$, where

\[
	l'(q) = \left\{\begin{array}{lr}
				s' & \textmd{if } p = q, \\
				l(q) &\textmd{if } p \neq q.
			\end{array} \right.
\]
In this case we write $\sigma \xrightarrow{t_{p,s}} \sigma'$.
We write $\longrightarrow^*$ to denote the transitive reflexive
closure of $\longrightarrow$.
A state $\sigma$ is called a \emph{stopped state} when there is no transition
$t$ such that $t$ is enabled in $\sigma$. We say that $t_1$ and $t_2$
may be \emph{co-enabled} if there is some state $\sigma \in \textit{State}$
such that both $t_1$ and $t_2$ are enabled in $\sigma$.

In any state $\sigma = (l, g)$, let
$\textit{next}(\sigma, p) = t_{p,l(p)}$ denote the unique next transition
to be executed by process $p$, and let
\[
	\textit{enabled}(\sigma) = \{t_{p,s} \in \mathcal{T} \mid
	p \in \mathcal{P} \wedge t_{p,s} = \textit{next}(\sigma, p)
	\wedge t_{p,s} \text{ is enabled in } \sigma\}
\]
denote the set of enabled transitions that can be executed from $\sigma$.
For any transition $t_{p,s}$, let $\textit{proc}(t_{p,s}) = p$
denote the unique process executing that transition.

\subsection{Systems}
The behaviour of an entire \emph{transition system} is
specified with the tuple $A = (\textit{State}, \Delta, \sigma_0)$,
where $\Delta \in State \to \mathbb{P}(State)$
is the \emph{transition relation} defined by
\[
	\sigma' \in \Delta(\sigma) \iff
	\exists t \in \mathcal{T}. \ \sigma \xrightarrow{t} \sigma'.
\]
Note that transitions are also known as models,
so model checking is the process of deciding
whether a given transition system has certain
properties.

\section{Model Checking}
This section specifies exactly what I mean by
a model checking algorithm, gives a simple
example, and introduces some concepts used
to tackle the state explosion problem.

\subsection{Definition of Model Checking}
Model checking is the process of deciding
whether a
given transition system is \emph{error-free} and
\emph{deadlock-free}.

To specify exactly what it means for a system to
be error-free, we extend each transition system
with a relation
$\textit{err} : \mathcal{S} \to \mathbb{B}$, which states
whether, in some given state, a particular
thread has encountered an error.

We say that a transition system
$A = (\textit{State}, \Delta, \sigma_0, \textit{err})$
is
\emph{error-free} if
\[
	\forall\, (l, g) \in \textit{State}. \ \sigma_0 \longrightarrow^* (l, g)
	\implies \forall p \in \mathcal{P}.\ \neg\,\textit{err}(l(p));
\]
that is, if for every state $\sigma$ reachable from the initial state
$\sigma_0$, no thread in $\sigma$ is in an error state.

We say that a transition system
$A = (\textit{State}, \Delta, \sigma_0, \textit{err})$
is deadlock-free if
\begin{align*}
	\forall\, (l, g) \in \textit{State}. \;\, (&\sigma_0
	 \longrightarrow^* (l, g))
	\wedge (\neg \exists t \in \mathcal{T}.\;
	t \in \textit{enabled}((l, g)) \\
	&\implies \forall t_{p,s} \in \mathcal{T}.\;
	(l(p) = s \implies \forall g' \in \mathcal{G}. \;
		t_{p,s}(g')\!\uparrow),
\end{align*}

where $\uparrow$ denotes that a partial function is
undefined at the given value.
That is, for all stopped states $\sigma$ reachable from
the initial state $\sigma_0$, if the shared store of
$\sigma$ was different, it would still be a stopped state.

For our purposes, a model checking algorithm is an
algorithm that takes a transition system and returns
two Boolean results, indicating whether that system
is error-free and deadlock-free. In general, more
powerful concepts of model checking are possible,
but the algorithms used in this project sacrifice
this expressive power for performance.

\subsection{Simple Model Checking} \label{sec:simple-model-checking}
As the definitions of both error-free and deadlock-free
can be expressed in the form
\[
	\forall \sigma \in \textit{State}.\;\, \sigma_0 \longrightarrow^* \sigma
	\implies \Phi (\sigma)
\]
for some predicate $\Phi$, the simplest strategy is to
perform an exhaustive search of the reachable state space,
checking whether $\Phi$ holds at each state encountered.

In practice, algorithms are used which decide both
properties in one exploration of the reachable state space,
but separate algorithms for each are presented
below for clarity.

\subsubsection{Detecting Errors}
When deciding whether the transition system
is error-free, we have
\[
	\Phi((l, g)) = \forall p \in \mathcal{P}.\; \neg\,err(l(p)),
\]
so the algorithm in figure~\ref{fig:simple-error-free}
decides whether a given transition system
is error-free. At each
state encountered, beginning with $\sigma_0$, first $\Phi$
is checked to hold, then recursive calls implement a
depth-first search of the reachable state space.

\begin{figure}
	\begin{algorithmic}[1]
	\Procedure{SimpleErrorFree}{$\sigma$}
	\Let{(l, g)}{$\sigma$}
	\ForAll{$p \in \mathcal{P}$}
		\If{$err(l(p))$}
		\Return \textit{false}
		\EndIf
	\EndFor \ForAll{$\sigma' \in \{\sigma' \mid \sigma \xrightarrow{t} \sigma' \wedge t \in \textit{enabled}(\sigma)\}$}
		\If{$\textbf{not } \textsc{SimpleErrorFree}(\sigma')$}
		\Return \textit{false}
		\EndIf
	\EndFor
	\State \Return \textit{true}
	\EndProcedure
	\State
	\State Initially: $\textsc{SimpleErrorFree}(\sigma_0)$
	\end{algorithmic}
	\caption{Algorithm deciding if a transition system is error-free}
	\label{fig:simple-error-free}
\end{figure}

\subsubsection{Detecting Deadlocks}
When deciding whether the transition system
is deadlock-free, we have
\[
	\Phi((l, g)) =\neg \exists t \in \mathcal{T}.\;
	\exists\sigma' \in \textit{State}.\;(l, g) \xrightarrow{t} \sigma'
	\implies \forall p \in \mathcal{P}.\;\textit{val}(l(p)),
\]
so figure~\ref{fig:simple-deadlock-free} below decides whether a given transition system
is deadlock-free. As $\Phi$ can only fail to hold
at stopped states, it is first
checked whether the state has any
transitions that can be executed. If not, then
$\Phi$ is checked to hold, else again recursive calls implement a
depth-first search of the reachable state space.
The predicate $\textit{val}~:~\mathcal{S} \to \mathbb{B}$
is true for threads which cannot proceed, no matter the
shared state, which is easily decidable in practice.

\begin{figure}
	\begin{algorithmic}[1]
		\Procedure{SimpleDeadlockFree}{$\sigma$}
		\Let{(l, g)}{$\sigma$}
		\Let{SuccessorStates}{$\{\sigma' \mid \sigma \xrightarrow{t}
			\sigma' \wedge t \in \textit{enabled}(\sigma)\}$}
		\If{$\text{SuccessorStates} = \emptyset$}
			\ForAll{$p \in \mathcal{P}$}
				\If{$\textbf{not } val(l(p))$} \Return \textit{false}
				\EndIf
			\EndFor
			\State \Return \textit{true}
		\Else
			\ForAll{$\sigma' \in \text{SuccessorStates}$}
				\If{$\textbf{not } \textsc{SimpleDeadlockFree}(\sigma')$}
					\Return \textit{false}
				\EndIf
			\EndFor
		\State \Return \textit{true}
		\EndIf
		\EndProcedure
		\State
		\State Initially: $\textsc{SimpleDeadlockFree}(\sigma_0)$
	\end{algorithmic}
	\caption{Algorithm deciding if a transition system is deadlock-free}
	\label{fig:simple-deadlock-free}
\end{figure}

\subsection{Partial-Order Reduction}
Although such na\"{\i}ve algorithms as those given
in section~\ref{sec:simple-model-checking}
are correct, due to the \emph{state explosion problem},
more efficient algorithms have been developed.
\emph{Partial-order reduction} is a category of
techniques which aim to reduce the size of the
state space explored. Some concepts in
partial-order reduction are introduced in
this section.

\subsubsection{Independence} \label{sec:independence}
In plain English, two transitions are independent if executing one
can never disable the other, and if they are commutative.

More formally, suppose that $\mathcal{T}$ is the set of
transitions for some transition system, and
that $I \subseteq \mathcal{T} \times \mathcal{T}$
is a reflexive and symmetric relation. We say
that $I$ is a valid \emph{independency relation}
whenever the following two conditions hold for
all $(t_1, t_2) \in I$:
\begin{enumerate}
	\item for all states $\sigma \in \textit{State}$,
		if $t_1$ is enabled in $\sigma$ and
		$\sigma \xrightarrow{t_1} \sigma'$, then
		$t_2$ is enabled in $\sigma$ if and only if
		$t_2$ is enabled in $\sigma'$; and
	\item for all states $\sigma \in \textit{State}$,
		if both $t_1$ and $t_2$ are enabled in $\sigma$
		then there are unique states $\sigma_1$, $\sigma_2$ and
		$\sigma'$ such that
		$\sigma \xrightarrow{t_1} \sigma_1 \xrightarrow{t_2} \sigma'$
		and
		$\sigma \xrightarrow{t_2} \sigma_2 \xrightarrow{t_1} \sigma'$.
\end{enumerate}

\begin{figure}[h]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics*[width=0.45\textwidth]{independence1}
		\caption{Transitions $t_1$ and $t_2$ are independent if
			they commute and cannot disable one another.}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics*[width=\textwidth]{independence2}
		\caption{If $t_1$ disables $t_2$ then $t_1$ and $t_2$
			are not independent.}
	\end{subfigure}
	\quad
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics*[width=\textwidth]{independence3}
		\caption{If $t_1$ and $t_2$ do not commute then they
			are not independent.}
	\end{subfigure}
	\caption{Illustration of transition independence}
	\label{fig:independence}
\end{figure}
Two transitions $t_1$ and $t_2$ are said to be \emph{independent}
if there is a valid independency relation $I$ such that $(t_1, t_2) \in I$.
If $t_1$ and $t_2$ are not independent, they are said to be \emph{dependent}.

\subsubsection{Persistent Sets}
Partial-order reduction algorithms typically 
perform a depth-first
search of the reachable state space
(as the algorithms in
section~\ref{sec:simple-model-checking} do), but at each
state $\sigma$ they explore only a subset
$T \subseteq \textit{enabled}(\sigma)$. Of course, the subsets
$T$ must be computed in a way such that the search
is still sound --- it must guarantee to
produce the correct result.

One partial-order reduction technique for
computing such sound subsets is known
as the \emph{persistent set} technique. Informally,
a subset of transitions $T$ from a state $\sigma$
is called \emph{persistent in $\sigma$} if every
possible transition $t \not \in T$ that is reachable
from $\sigma$ by exploring only transitions not in
$T$ does not interact with the transitions
in $T$.

Formally, a subset of transitions $T \subseteq \mathcal{T}$
from a state $\sigma_1$
is called \emph{persistent in $\sigma_1$} if and only if
for all sequences of transitions
\[
	\sigma_1 \xrightarrow{\ t_1\ } \sigma_2 \xrightarrow{\ t_2\ } \ldots
	\xrightarrow{t_{n-1}} \sigma_n,
\]
if for all $1 \leq i < n$, $t_i \not \in T$ then each $t_i$ is
independent with every transition in $T$.

\begin{figure}[h]
	\centering
	\includegraphics[height=9cm]{persistent1}
	\caption{Diagram of an example state space, for
		the illustration of persistent sets.}
	\label{fig:persistent}
\end{figure}

For illustration, in the state space shown in
figure~\ref{fig:persistent}, $T = \{t_1\}$ is
a persistent set in the initial state, as each
transition reachable following only transitions
not in $T$ is independent with $t_1$.
Likewise, $T = \{t_2, t_3\}$ is
a persistent set in the initial state, as the
only transition reachable from the initial state
without executing any transitions in $T$ is $t_1$,
and $t_1$ is independent with both $t_2$ and $t_3$.
However, $T = \{t_2\}$ is not a persistent set in the
initial state, as $t_2$ and $t_3$ are not independent.
As we might hope, if we choose only transitions
in a persistent set (either $\{t_1\}$ or $\{t_2, t_3\}$)
from the initial state to begin
an otherwise exhaustive search of the state space, then
both stopped states are reachable.

Indeed, it can be shown that if a stopped state is reachable by an exhaustive
search of the state space, then by computing a persistent set $T$ at
each state $\sigma$ and exploring only those transitions from
$\sigma$ that are in $T$, that stopped state is still reachable
(cf. Theorem 4.3 in \textbf{Godefroid thesis}). It can also be shown
that by exploring only transitions in persistent sets, all reachable
local states of each process are explored (cf. Theorem 6.14 in
\textbf{Godefroid thesis}). Therefore, if we are interested in
the definition of model checking given in 
section~\ref{sec:simple-model-checking}, then it is sufficient
to explore transitions from the persistent sets of the states we
encounter.

\subsubsection{Sleep Sets}

Another technique for addressing the state explosion
problem is that of \emph{sleep sets}. Suppose that
we are at state $\sigma_0$ when performing a search
of the state space (see figure~\ref{fig:sleep}).
Suppose that the first transition
we explore is $t_1$, reaching $\sigma_1$,
and the second is $t_2$, reaching $\sigma_2$.
If $t_1$ and $t_2$ are independent, then there
is a state $\sigma'$ reachable by both
$\sigma \xrightarrow{t_1} \sigma_1
\xrightarrow{t_2} \sigma'$ and
$\sigma \xrightarrow{t_2} \sigma_2
\xrightarrow{t_1} \sigma'$.
Any stopped state reachable from $\sigma'$
is reachable from $\sigma_1$, as $\sigma_1$
is reachable from $\sigma'$. By assumption,
when the search is at $\sigma_2$, we have already
explored all the stopped states reachable from
$\sigma_1$ (the dotted area in figure~\ref{fig:sleep}),
in particular those reachable from
$\sigma'$. Therefore, there is no need to
explore $t_1$ from $\sigma_2$.

In fact, there is no need to consider executing
$t_1$ in our recursive exploration from $\sigma_2$
until we execute some transition $t'$ which is
\emph{not} independent of $t_1$, as after
the execution of $t'$, exploring $t_1$ may
now lead to an unexplored area of the state space.

A \emph{sleep set} for a particular state
is then the set of transitions which, when
explored, is guaranteed by the above
argument to lead to an already-explored
section of the state space. The technique of
sleep sets is orthogonal to the technique
of persistent sets -- there is no interaction
between the two techniques, so implementing
both simultaneously can be achieved with
no extra difficulty.

\begin{figure}[h]
	\centering
	\includegraphics[height=8cm]{sleep}
	\caption{Diagram illustrating of the theory
		of sleep sets.}
	\label{fig:sleep}
\end{figure}

\section{Dynamic Partial-Order Reduction}
This section introduces the DPOR algorithm,
which is the focus of the project.

\subsection{Background Definitions}
The DPOR makes use of several new concepts,
which are presented here.

\subsubsection{Transition Sequences}
Instead of keeping track of just the current state, as
the algorithms in section~\ref{sec:simple-model-checking}
did, the DPOR algorithm keeps track of a \emph{transition
sequence} $\pi \in \mathcal{T}^*$. This transition
sequence is implicitly executed from the initial state of
the transition system, $\sigma_0$, so that in practice,
$\pi = t_0, t_1, \ldots, t_{n-1}$ determines a sequence of states
$\sigma_0, \sigma_1, \ldots, \sigma_n$ such that
\[
	\sigma_0 \xrightarrow{\ t_0\ } \sigma_1 \xrightarrow{\ t_1\ }
	\ldots \xrightarrow{t_{n-1}} \sigma_n.
\]

Given a transition sequence $\pi = t_0, t_1, \ldots, t_{n-1}$,
we use the following notation:
\begin{itemize}[label={}]
	\newcommand{\defsindent}{3.5em}
	\item{\makebox[\defsindent]{\hfill$\pi_i$}
		--- the transition $t_i$;}
	\item{\makebox[\defsindent]{\hfill$\pi.t$}
		--- the transition sequence $\pi$ extended with
		an additional transition $t$;}
	\item{\makebox[\defsindent]{\hfill$\textit{dom}(\pi)$}
		--- the set $\{i \in \mathbb{N} \mid 0 \leq i < n \}$;}
	\item{\makebox[\defsindent]{\hfill$\textit{pre}(\pi, i)$}
		--- the state $\sigma_i$ (i.e. the
		state immediately before transition $t_i$); and}
	\item{\makebox[\defsindent]{\hfill$\textit{last}(\pi)$}
		--- the final state, $\sigma_n$.}
\end{itemize}

\subsubsection{The Happens-Before Relations}\label{sec:happens-before}

Suppose that two adjacent transitions, $\pi_i$ and $\pi_{i+1}$,
are swapped in a
transition sequence, $\pi$, to give a new
transition sequence, $\pi'$. Suppose further
that $\pi_i$ and $\pi_{i+1}$ are independent.
Since $\pi_{i+1}$ is enabled in $\textit{pre}(\pi, i+1)$,
$\pi_{i+1}$ must also be enabled in $\textit{pre}(\pi, i)$,
as independent transitions cannot disable one another.
If $\pi_{i+1}$
is executed in $\textit{pre}(\pi, i)$,
it results in a state in which $\pi_i$
can be executed, as it cannot be disabled by $\pi_{i+1}$.

Because independent transitions commute, the state reached by
executing $\pi_i$ followed by $\pi_{i+1}$ is the same
as the state reached by executing $\pi_{i+1}$ followed
by $\pi_i$.
So $\textit{pre}(\pi, i+2) = \textit{pre}(\pi', i+2)$. Since 
for all $j \geq i + 2$, $\pi_j = \pi'_j$, it follows that
$\textit{last}(\pi) = \textit{last}(\pi')$. So swapping
a pair of adjacent transitions results in the same
final state if the transitions are independent.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{seqequiv}
	\caption{Illustration}
	\label{fig:sequence-equivalence}
\end{figure}

Such equivalent interleavings
of transitions are precisely what we are interested in.
We can think of a transition sequence as representing
the equivalence class of transition sequences obtained
by swapping adjacent pairs of independent transitions;
if we can identify that two transition sequences are
members of the same equivalence class, we need only
explore one.

To help us to reason about such equivalence classes,
we will use a ``happens-before" relation to identify
when one transition cannot be swapped with another.
In particular, we define the \emph{happens-before}
relation $\longrightarrow_\pi$ for a transition
sequence $\pi$ to be the smallest relation on
$\textit{dom}(\pi)$ such that
\begin{enumerate}
	\item if $i \leq j$ and $\pi_i$ is dependent with
		$\pi_j$ then $i \longrightarrow_\pi j$; and
	\item $\longrightarrow_\pi$ is transitively closed.
\end{enumerate}

By construction, the happens-before relation is a
partial-order relation on the transitions in $\pi$,
and $\pi$ is one of the linearisations of this
partial order. The other linearisations of the
partial order are the equivalent sequences of
transitions which can be obtained by swapping
adjacent pairs of independent transitions in $\pi$.

The DPOR algorithm also makes use of a variant of the
happens-before relation.
Informally, if $(i, p)\!\hookrightarrow_\pi$ then
in every linearisation $\hat{\pi}$ of the
happens-before relation (that is, every sequence
obtained by swapping pairs of adjacent,
independent transitions in $\pi$), the next
transition that process $p$ executes in
state $\textit{pre}(\pi, i)$ is executed
at some point before $\textit{last}(\hat{\pi})$.
Formally, 
if $i \in \textit{dom}(\pi)$ and $p \in
\mathcal{P}$ then we
say that $(i, p)\!\hookrightarrow_\pi$ if
either
\begin{enumerate}
	\item $\textit{proc}(\pi_i) = p$; or
	\item $\exists k \in \textit{dom}(\pi).\;
	i < k \,\wedge\, \textit{proc}(\pi_k) = p
	\,\wedge\, i \longrightarrow_\pi k $.
\end{enumerate}


\subsection{The DPOR Algorithm}

\subsubsection{Overview}

The Dynamic Partial-Order Reduction (DPOR) algorithm
dynamically uses information gathered during its
exploration of a search space to explore only
transitions in a persistent set from each state
encountered on the search -- a partial-order
reduction technique. The idea is that the
extra information which is available because
the persistent sets are created dynamically
allows smaller (and therefore more efficient)
persistent sets to be chosen than an algorithm
computing its persistent sets statically.

In particular, at each state $\sigma$, the
next transition
to execute, $t$, is chosen at random, and it is
initially assumed that that transition will
be the only transition to be explored from
that state. However, this is a persistent
set only if $t$ is
independent of every transition reachable
by exploring any transitions
from $\sigma$ other than $t$. Clearly,
this isn't true in general, so, at each
state $\sigma'$ encountered beyond
$\sigma$, if there is a transition
$t' \in \textit{enabled}(\sigma')$
such that $t'$ is dependent with
$t$ and $t'$ may appear before $t$
in an exploration from $\sigma$, then
a backtracking point is added at
$\sigma$ to ensure that such an
exploration is made. Over time,
the addition of such backtracking points
ensures that the set of transitions
explored from each state is a
persistent set.

\begin{figure}
	\begin{algorithmic}[1]
		\Procedure{Explore}{$\pi$}
		\Let{$\sigma$}{$\textit{last}(\pi)$}
		\ForAll{$p \in \mathcal{P}$}
			\State \Call{UpdateBacktrackSets}
					{$\pi,\, \textit{next}(\sigma, p)$}
		\EndFor
		\If{$\textit{enabled}(\sigma) \neq \emptyset$}
		\Let{$t$}{any $t \in \textit{enabled}(\sigma)$}
		\Let{$\textit{backtrack}(\sigma)$}{$\{t\}$}
		\Let{$\textit{done}(\sigma)$}{$\emptyset$}
		\While{$\textit{done}(\sigma)
			 \neq \textit{backtrack}(\sigma)$}
		\Let{$t$}{any $t \in (\textit{backtrack}(\sigma)
			\setminus \textit{done}(\sigma))$}
		\State add $t$ to $\textit{done}(\sigma);$
		\State \Call{Explore}{$\pi.t$}

		\EndWhile
		\EndIf
		\EndProcedure
		\State
		\Procedure{UpdateBacktrackSets}{$\pi,\, t_{p,s}$}
			\Let{$D$}{$\{i \in \textit{dom}(\pi) \mid
			\pi_i \text{ is dependent with } t_{p,s}
			\text{ and } (i, p)\!\not \hookrightarrow_\pi \}$}
			\If{$D \neq \emptyset$}
			\Let{$\sigma_d$}
				{$\textit{pre}(\pi,\text{max}(D))$}
			\If{$\textit{next}(\sigma_d, p)
				\in \textit{enabled}(\sigma_d)$}
			add $\textit{next}(\sigma_d, p)$
			to $\textit{backtrack}(\sigma_d)$
			\Else {
				add all of $\textit{enabled}(\sigma_d)$
				to $\textit{backtrack}(\sigma_d)$
			} \EndIf
			\EndIf
		\EndProcedure
		\State
		\State Initially: \Call{Explore}{$\emptyset$}
	\end{algorithmic}
	\caption{The DPOR algorithm}
	\label{dpor-outline} 
\end{figure}

\subsubsection{Line-By-Line Explanation}
\begin{description}
	\item[Lines 1--2] The \textsc{Explore} procedure implements
	dynamic partial-order reduction. The transition sequence
	$\pi$ which is passed as its argument specifies the state
	from which the exploration is to take place, $\sigma$.

	\item[Lines 3--4] First, any necessary backtracking points
	which can be identified from this state are added to the
	\textit{backtrack} sets of previous states. Backtracking
	points can be identified using information from each possible
	transition from this state, so the \textsc{UpdateBacktrackSets}
	procedure is called with each possible next transition as an
	argument.

	\item[Line 5] Having updated the \textit{backtrack} sets
	of previous states, the search continues its exploration,
	but only if there are any enabled transitions to explore.

	\item[Lines 6--8] The recursive exploration proceeds by
	maintaining two sets of transitions: \textit{backtrack}
	and \textit{done}. The \textit{backtrack} set contains
	those transitions from $\sigma$ which should be explored,
	and is initialised to any enabled transition. Recursive calls to
	\textsc{Explore} may add more transitions to \textit{backtrack}.
	The \textit{done} set contains those transitions
	in \textit{backtrack} that have been explored.

	\item[Line 9] The search from $\sigma$ halts when every
	transition that needs to be explored (is in \textit{backtrack})
	has been explored (is in \textit{done}).

	\item[Lines 10--12] If there is some transition that needs
	exploring, \textsc{Explore} is recursively called with that
	transition extending the current transition sequence $\pi$,
	that transition is added to \textit{done}.

	\item[Line 14] The \textsc{UpdateBacktrackSets} procedure
	can use the information that a given transition, $t_{p,s}$ can be
	executed in $\sigma$ to add necessary backtracking points.

	\item[Line 15] The set $D$ contains the indices $i$ of the
	transitions $\pi_i$ in the sequence such that $\pi_i$ and
	$t$ are dependent, and $(i, p)\!\not \hookrightarrow_\pi$.
	This second condition means that $t_{p,s}$ is \emph{not}
	guaranteed to happen after $\pi_i$, so $D$ is the set
	of transitions which may have a race condition with
	$t_{p,s}$.

	\item[Line 16] If there are no transitions which have
	a race condition with $t_{p,s}$, then no backtracking
	points need to be added: as far as we know from $t_{p,s}$,
	if we had chosen any other
	choices of transitions to explore previously, we would
	have ended up with a transition sequence in the same
	equivalence class as $\pi$.

	\item[Line 17] If there are transitions in $\pi$
	which have a race condition with $t_{p,s}$, then
	it is sufficient to add a backtracking point at
	the state immediately before
	the most recent of these;
	the necessary backtracking
	points for the earlier states will be added
	by later (or previous) calls of \textsc{Explore}.

	\item[Line 18] We know there is a race condition
	between $t_{p,s}$ and $\textit{next}(\sigma_d,p)$, so
	we want to explore another transition sequence from
	$\sigma_d$ in which $\textit{next}(\sigma_d,p)$
	appears before $t_{p,s}$. If $\textit{next}(\sigma_d,p)$
	is enabled in $\sigma_d$, then we certainly explore such
	a transition sequence by immediately exploring
	$\textit{next}(\sigma_d,p)$ from $\sigma_d$.

	\item[Line 19] However, if we cannot immediately
	explore $\textit{next}(\sigma_d,p)$
	from $\sigma_d$ because it is not enabled, then it is
	not obvious which transitions to explore from $\sigma_d$
	 to achieve
	a transition sequence in which $\textit{next}(\sigma_d,p)$
	appears before $t_{p,s}$. The algorithm presented here
	does not attempt to narrow down which transitions might
	lead to such a transition sequence, and instead plays it
	safe and explores them all.

	\item[Lines 21] To perform model checking, the state space
	is explored from the initial state, $\sigma_0$, which is
	reached from $\sigma_0$ by executing no transitions, so
	\textsc{Explore} is initially called on the empty
	sequence of transitions, $\emptyset$.

\end{description}

\subsubsection{Example Execution}
An example execution, on the same example as
figure~\ref{fig:sdpor-motivation}.

\chapter{Implementation}
\textbf{This chapter explains how the mathematical
	psuedocode in the literature was translated
	into a working program which model checks a
	given source file.}

\section{Design of the Software}

The project was implemented in OCaml, which
offers a few key advantages:
\begin{itemize}
	\item a clean, functional
	style, which is particularly appropriate for
	this project, given its mathematical nature;
	\item a strong, safe type system, which makes
	impossible a large class of runtime errors;
	\item parametric polymorphism, a module system,
	and other higher-order programming features,
	facilitating abstraction and generalisation; and
	\item established libraries, providing efficient
	implementations of basic data structures and algorithms.
\end{itemize}
I took particular advantage of the module system when
designing and implementing the project. OCaml modules
are used to encapsulate related definitions and hide
information -- \emph{signatures} are software interfaces
providing declarations,
and \emph{structures} are implementations which 
provide definitions. A structure \emph{matches} a
signature if it has a definition for each declaration
in the signature. \emph{Functors} can be thought of
as functions from structures to structures. If a
module matches a given signatures, then it can be
passed to a functor to give a new structure.

\begin{figure}
	\centering
	\begin{subfigure}{.4\textwidth}
		\centering
		\includegraphics[height=9cm]{interfaces}
		\caption{Diagram showing which signatures declare
			which other signatures as nested sub-structures.}
		\label{fig:interfaces}
	\end{subfigure}%
	\quad
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[height=12cm]{functors}
		\caption{Diagram showing generation of
			structures using functors. The blue files must be provided;
			the functors are part of this project; the remaining files
			are generated using the functors.}
		\label{fig:functors}
	\end{subfigure}
	\caption{Diagrams showing the interactions between
		the software modules.}
	\label{fig:design}
\end{figure}

In the project, there are five main signatures:
\begin{enumerate}
	\item a structure that matches \emph{Expression}
	contains information about the structure of expressions
	in the programming language of interest;
	\item a structure that matches \emph{Store}
	implements a store (i.e. a mapping from locations
	to expressions) for some implementation of \emph{Expression};
	\item a structure that matches \emph{Thread} contains
	the information about the semantics of expressions
	for a programming language;
	\item a structure that matches \emph{Program}
	contains declarations relating to
	transition systems as expressed in some programming
	language; and
	\item a structure that matches \emph{Checker}
	contains a function which performs model checking.
\end{enumerate}

While I did implement a simple programming language
(see section~\ref{sec:language}), the model checking
algorithms that are the focus of this project are of
course entirely independent of any particular language.
I therefore ensured that, in order to use
my project to perform model checking for programs
in any other language, the work that would have to
be done would be minimal. In particular, given implementations
of \emph{Expression} and \emph{Thread} (effectively
specifying the semantics of a programming language), my system uses
functors to automatically generate implementations of
\emph{Store}, \emph{Program} and \emph{Checker}, the
last of which can then be used to immediately perform
model checking. This process is shown explicitly in
figure~\ref{fig:functors}.

\section{Development of Project Language}\label{sec:language}
\textbf{This section explains how I designed
	and implemented the language in which the
	concurrent programs to be model checked
	are written.}

\subsection{Language Design}
\textbf{First, the language had to be designed.
	Explain design choices, give sense of what
	the language is, maybe an example program,
	and point to the appendix for a full definition.}

Although, as mentioned above, any programming language
could be used to express transition systems for model
checking, I chose to design and implement a simple
ML-like language, PL (Project Language), to avoid
the unnecessary complexity of implementing a full
industrial language.
The grammar for valid PL expressions is given in
figure~\ref{fig:pl-expressions}.

\begin{figure}
	\centering
	\setlength{\tabcolsep}{2pt}
	\begin{tabular}{rl}
		\textit{expr} $:=$ & \textit{int} $\vert$ \textit{bool} $\vert$ \textit{var} \\
		$\vert$ & \textit{local} $\vert$ \textit{global}
			$\vert$ \textit{spinlock} \\ 
		$\vert$ & \textit{expr op expr} $\vert$ $\neg\,$\textit{expr} \\
		$\vert$ & \textbf{skip} $\vert$ \textit{expr; expr}\\
		$\vert$ & \textbf{if} \textit{expr}
			\textbf{then} \textit{expr} \textbf{else} \textit{expr} \\
		$\vert$ & \textbf{while} \textit{expr}
			\textbf{do} \textit{expr} \\
		$\vert$ & \textit{expr} $:=$ \textit{expr} $\vert$ !\textit{expr} 
			$\vert$ \textbf{ref} \textit{expr} \\
		$\vert$ & \textbf{fn} \textit{var}
			$\Rightarrow$ \textit{expr} $\vert$ \textit{expr expr} \\
		$\vert$ & \textbf{let} \textit{var} $=$
			\textit{expr} \textbf{in} \textit{expr} \\
		$\vert$ & \textbf{let rec} \textit{var} $=$
			\textbf{fn} \textit{var} $\Rightarrow$
			\textit{expr} \textbf{in} \textit{expr} \\
		$\vert$ & \textbf{lock} \textit{expr} $\vert$
			\textbf{unlock} \textit{expr} \\
		$\vert$ & \textbf{cas}(\textit{expr}, \textit{expr}, \textit{expr}) \\
		$\vert$ & \textbf{error}(\textit{message}) \\
		& \\
		\textit{int} $\in$ & $\{\ldots, -1, 0, 1, \ldots\}$ \\
		\textit{bool} $\in$ & $\{\textbf{true}, \textbf{false}\}$ \\
		\textit{var} $\in$ & \textit{Variable}
	\end{tabular}
	\qquad\quad
	\begin{tabular}{rcl}
		\textit{op} $:=$ & $+$ &\\
		$\vert$ & $-$ &\\
		$\vert$ & $\ast$ &\\
		$\vert$ & $/$ &\\
		$\vert$ & $\%$ &\\
		$\vert$ & $>$ &\\
		$\vert$ & $<$ &\\
		$\vert$ & $=$ &\\
		$\vert$ & $\&$ &\\
		$\vert$ & $\mid$ &\\
		& &\\
		\textit{local} & $\in$ & \textit{Location} \\
		\textit{global} & $\in$ & \textit{Location} \\
		\textit{spinlock} & $\in$ & \textit{Location} \\
		& &\\
		\textit{Location} & $=$ & $\{a, b, c, \ldots\}^\ast$\\
		\textit{Variable} & $=$& $\{x, y, z, \ldots\}$ \\
	\end{tabular}
	\caption{The grammar for PL expressions.}
	\label{fig:pl-expressions}
\end{figure}

As required by the \emph{Thread} interface, each
PL thread has access to both its local store $s$
and the global store $g$. Both $s$ and $g$ are
maps from locations to expressions.
To express the semantics
of PL, then, we need to give whether or not an
expression $e$ reduces for local store $s$ and
global store $g$, and if it does, we need to give
the expression $e'$ that it reduces to in one ``step",
as well as any updates to $s$ and $g$ that result.
Note that each such ``step" is assumed to be atomic.
The semantics of PL expressions are as you would expect
(see \textbf{THE APPENDIX} for a full definition), but
the rules for the reduction of expressions that interact
with the global store $g$ are given in figure~\ref{fig:pl-rules}.

\begin{figure}
	\begin{gather*}
		\frac{g(\textit{spin}) = \textbf{true}}
			{(\textbf{unlock}\ \textit{spin},\ s,\ g)
				\longrightarrow (\textbf{skip},\ s,\
				g[\textit{spin} \mapsto \textbf{false}])} \\
		\\
		\frac{g(\textit{spin}) = \textbf{false}}
		{(\textbf{lock}\ \textit{spin},\ s,\ g)
			\longrightarrow (\textbf{skip},\ s,\
			g[\textit{spin} \mapsto \textbf{true}])} \\
		\\
		\frac{g(\textit{glob}) = e_\textit{curr}}
			 {(\textbf{cas}(\textit{glob},\, e_\textit{curr},\, e_\textit{new})
			 	,\ s,\ g) \longrightarrow (\textbf{true},\ s,
			 	\ g[\textit{glob} \mapsto e_\textit{new}])} \\
		\\
		\frac{g(\textit{glob}) \neq e_\textit{curr}}
			{(\textbf{cas}(\textit{glob},\, e_\textit{curr},\, e_\textit{new})
				,\ s,\ g) \longrightarrow
				(\textbf{false},\ s,\ g)} 
	\end{gather*}
	\caption{Rules for the atomic reduction of PL
		expressions that interact with the shared store.}
	\label{fig:pl-rules}
\end{figure}

There is no type system for PL. Although it would have been
beneficial to design and implement a type system for PL
(a type checker could catch type errors early, rather than a runtime
failure with non-obvious causes occurring), I thought that the total
time I would have spent on creating the type system would be more than
the time taken in dealing with the problems resulting from having
no type system. With the benefit of hindsight, I think this was the
correct decision.

\subsection{Parser}
On the other hand, I decided that investing time in creating a
parser for PL would pay itself off over the course of the project.
This allows me to write, for example,
\begin{align*}
	\texttt{let y = ref 1 in cas(Gx, 7 * !y - 3, 0)},
\end{align*}
instead of having to type the OCaml meta-language abstract
syntax tree representing the structure of the Pl expression:
\begin{align*}
	\texttt{Let (Ref (Integer}& \texttt{ 1), Cas (Global "x",
		Op (Op (Integer 7,} \\ \texttt{Mult,}& \texttt{ Deref (Var 0)), Minus, Integer 3),
		Integer 0))}.
\end{align*}
Writing such expressions is not only extremely time-consuming but
also error-prone. For the language to be of any practical use,
a parser is a necessity.

To create a parser for PL, two program generators were used:
\emph{OCamllex} and \emph{Menhir}. When presented with a
file giving a mapping from strings of characters to tokens,
OCamllex produces a lexical analyser, which converts a sequence
of characters into a sequence of tokens. When presented with a
grammar and a set of precedence rules, Menhir produces a parser,
which converts a sequence of tokens into an abstract syntax tree
of the grammar. These were used in conjunction, as shown in
figure~\ref{fig:parsing}, to produce a PL parser.

\begin{figure}
	\centering
	\includegraphics[height=10cm]{parsing}
	\caption{Diagram showing the flow and processing of
		information in the generation of a PL abstract
		syntax tree.}
	\label{fig:parsing}
\end{figure}

\subsection{Implementation}

\subsubsection{The Expression Module}
There are actually two datatypes which are used to
represent PL expressions in the
PL implementation of the \emph{Expression} module.
The first of these is the ``raw" expression, which
uses strings to represent variables (this is the
datatype that the parser produces). However,
manipulation of expressions is much easier when
variables are represented using \emph{de Bruijn indices},
which is the purpose of the second expression datatype.
This is the only one required by the \emph{Expression}
signature. A function exists to recursively convert
from the ``raw" datatype to this second datatype.

The remainder of the \emph{Expression} module
consists of auxiliary functions, which are
necessary for the implementation of later
modules, and either return a simple piece
of information or perform a simple manipulation.

\subsubsection{The Store Module}
As mentioned above, I implemented a functor
called \textbf{ListStore}, which, given a structure
matching the \emph{Expression} signature,
gives a structure matching the \emph{Store}
signature. The \emph{Store} implementation produced
represents stores using a simple list of
(location, expression) pairs. If a location
appears twice in the list, the occurrence
nearest the head of the list is taken to be
the correct value. A function is provided
which walks through from head to tail and
removes any stale pairs, which although
linear in time, reduces the space taken
by the list.

More efficient store implementations are of
course possible (for example, one based on
a hash table rather than a list), but
I went for the simplest reasonable
implementation I could think of, to
ensure correctness.

The only other function of note is
\texttt{get\_fresh\_loc},
which queries the \emph{Expression} \texttt{random\_loc}
function until it returns a result which is
unbound in the store. This is used to generate a location
to use when a new local reference is created using the
\textbf{ref} keyword.

\subsubsection{The Thread Module}
The \emph{Thread} module consists mostly of
the \texttt{next\_step} and \texttt{next\_transition}
functions. The \texttt{next\_step} function uses pattern
matching on PL expressions to determine what the next atomic
operation, if any, to be executed in the reduction
of the expression is. There is a close correspondence
between the
rules of the operational semantics (see \textbf{APPENDIX})
and the different cases in the pattern matching. This is
because each rule specifies that an expression of a
particular form, perhaps with some conditions on the
states of the local and shared stores, reduces in a
particular way. For illustration, one rule and its
corresponding pattern matching case is shown in 
figure~\ref{fig:step-rule-correspondence}.

\begin{figure}
	\begin{subfigure}{\textwidth}
		\begin{gather*}
		\frac{}
		{(\textbf{if true then } e_2 \textbf{ else }
				e_3,\,s,\,g) \rightarrow (e_2,\,s,\,g)}
		\qquad
		\frac{}
		{(\textbf{if false then } e_2 \textbf{ else }
			e_3,\,s,\,g) \rightarrow (e_3,\,s,\,g)} \\
		\\
		\frac{(e_1,\,s,\,g) \rightarrow
			(e_1^{\prime},\,s^{\prime},\,g^{\prime})}
		{(\textbf{if } e_1 \textbf{ then }
			e_2 \textbf{ else } e_3,\,s,\,g)
			\rightarrow (\textbf{if } e_1^{\prime}
			 \textbf{ then }
			e_2 \textbf{ else } e_3,\,
			s^{\prime},\,g^{\prime})}
		\end{gather*}
		\caption{Rules relating to
			the reduction of if-then-else expressions.}
	\end{subfigure}
	\\ \\ \\
	\begin{subfigure}{\textwidth}
		\lstinputlisting[frame=none, firstline=59, lastline=63]
			{../src/pl_thread.ml}
		\caption{Pattern matching case for if-then-else expressions.
			If \texttt{e1} is a Boolean, then reduce to \texttt{e2} or
			\texttt{e3},
			otherwise use a recursive call to try to reduce \texttt{e1}.}
	\end{subfigure}
	\caption{A concrete illustration of the correspondence
		between the rules of the operational semantics
		and the pattern matching statements of the
		\texttt{next\_step} function.}
	\label{fig:step-rule-correspondence}
\end{figure}

As a transition for a thread is defined as a
finite sequence of \emph{invisible} local
operations, followed by one \emph{visible} operation
which accesses the shared store, the \texttt{next\_transition}
function simply repeatedly calls the \texttt{next\_step}
function, reducing the initial expression and modifying
the local store, until either the resulting step accesses the shared
store, or no further reduction is possible. Noting that this
process is independent of the object language (PL, in this case),
the most general implementation of this project would require
only the  \texttt{next\_step} function to be provided by
the user, with a functor being used to automatically
produce the \texttt{next\_transition} function. However,
in this case, I decided that the additional design complexity
cost was not worth the slight improvement to the abstraction
of the project.

\subsubsection{The Program Module}
The \emph{Program} module is a small functor which,
when given a \emph{Thread} implementation, provides 
the few additional definitions and auxiliary
functions necessary to fully represent
transition systems (cf. section~\ref{sec:background-defs}.)
In particular, a state $\sigma \in \textit{State}$ is
represented by an instance of the \texttt{state} type:
\lstinputlisting[frame=none, firstline=107, lastline=107]
	{../src/interfaces.ml}

\subsubsection{Testing}
To ensure that the implementation of the PL
language was correct, I wrote unit tests for
the \texttt{next\_transition} and
\texttt{next\_step} functions. These tests
consisted of a series of inputs and expected outputs
to these functions. My aim in writing these tests
was to ensure that most of the code was covered,
so that any obvious bugs could be corrected
immediately. However, I did not invest time
making these tests very rigorous for two reasons:
any bugs found later would be relatively straightforward
to correct; and getting the PL implementation
fully correct was not the aim of the project, but
just a necessary step before the main work on the
model checking algorithms.


\section{Simple Model Checker}

In order to ensure the correctness of my implementation
of the DPOR algorithm, to have a baseline to use
for performance comparison, and to reaffirm my understanding
of the implementation of model checking, I first wrote the
SimpleChecker functor, which takes an implementation of
\emph{Program} as its input, and outputs a structure
which includes the definition of
a function that performs model checking by performing
an exhaustive depth-first search of the state space --
the simplest possible model checking algorithm.

Include why \texttt{check local} was put in Thread.

\subsection{Implementation}
\begin{itemize}
	\item Functor from programs to Checkers (independent of language)
	\item Present pseudo-code which is fairly close to OCaml but
		without too much detail.
	\item In particular explain how errors and deadlocks are found
	\item Aim is to convince reader that definitely correctly
		implemented.
\end{itemize}

\subsection{Testing}
\begin{itemize}
	\item Some test cases to sanity check
	\item In practice did error first and deadlock later,
		but not super relevant
	\item Idea is to use this to check DPOR, because this is so 
		obviously correct
\end{itemize}

\section{Dynamic Partial-Order Reduction}
\textbf{This section explains how the DPOR
	alogorithm was implemented in practice.}

\subsection{Implementation}
\subsubsection{Clock Vectors}
\textbf{Used to decide the happens-before relation.}
\subsubsection{Independence}
\textbf{Keep track of last transition that accessed
	each global object.}

\subsection{Locks}
\textbf{Difficulties faced adding locks;
	how they were overcome.}

Problem: many algorithms use locks. While \texttt{while locked do skip done}
is good enough, it results in a cycle in the state space, which causes
non-termination of DPOR.

Solution: add locks. New problem: no distinction between
transition not existing and not being enabled. Without
locks, all existing transitions were enabled; no longer
the case.

Solution: pass around Boolean stating whether or not the transition
is enabled.

\section{Stateful Model Checking}
When explored a state space, the same state may be
encountered twice. However, the part of the state
space from that state onwards need only be explored
once. An obvious technique for improving the
time complexity of a model checking algorithm
at the cost of using extra space is to notice
when we visit a state twice, by making a note
of which states we have explored.

\subsection{Simple Model Checker}
\textbf{Straight-forward implementation for simple
	checker. Give new algorithm.}

Applying this technique to the Simple model
checking algorithm is straightforward: whenever
we visit a state, add it to some data structure,
then don't re-explore those states which are
present in that data structure. As the operations
of checking if a state is in the data structure
and of adding a state to the data structure will
be used at every state visited, we would like them
to be as efficient as possible. The typical costs
for these operations for a hash table are both
constant, so I used the OCaml library implementation
of hash tables for this function.

The pseudocode for the Simple model checking
algorithm with this technique applied is given
in \textbf{FIGURE TO DO}.

\subsection{Dynamic Partial-Order Reduction}
Although it is not obvious, na\"{\i}vely
applying this technique results in an
unsound model checking algorithm. Instead,
a new algorithm, SDPOR, is used.

\subsubsection{Na\"{\i}ve Approach}
\textbf{What this approach is; why it doesn't
	work; example.}

Suppose that we applied the approach
given above to the DPOR algorithm,
resulting in the pseudocode in \textbf{FIGURE TO DO}.
The following example shows that the resulting
algorithm is not sound.

Consider the execution of the algorithm on example
 shown in figure~\ref{fig:sdpor-motivation},
at the stage when all the stages in grey have been
explored and so are present in the hash table,
and the current transition sequence is the first
transition of thread 1 followed by the first
transition of thread 0. As this stage, the state
with the bold outline is encountered, which is
present in the hash table, so no exploration is
made, and as no backtracking points have been added
at either of the previous stages, the algorithm
terminates. However, this leaves part of the state
space entirely unexplored (the dotted section in
the diagram), meaning that the algorithm is unsound:
any error or deadlock in a dotted state would go
undetected.

The reason for the unsoundness is the failure to
introduce backtracking points when necessary. If the
search had not been cut short, then a backtracking point
would have been added in the second state, causing the
dotted section to be explored.

\begin{figure}

	\begin{subfigure}{\textwidth}
		\begin{tabular}{lp{0cm}l}
		Thread 0: && Thread 1: \\
		\qquad\texttt{x := !x} $\times$ \texttt{2;}
			&& \qquad\texttt{x := !x} $\times$ \texttt{3;} \\
		\qquad\texttt{x := !x + 1} && \qquad\texttt{x := !x + 4}
		\end{tabular}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{sdpor}
	\end{subfigure}
	\caption{SDPOR motivation}
	\label{fig:sdpor-motivation}
\end{figure}




\subsubsection{Stateful Dynamic Partial-Order Reduction}
The most obvious solution to this problem is to simply
add all possible backtracking points whenever we cut
short a search. Although this does guarantee that all
necessary backtracking points are added (and therefore
that the algorithm is sound), in practice, the cost
of exploring \emph{all} possible previous backtracking points
dominates the benefit of cutting short searches when we
encounter an already-visited state. To make use of this
technique, we need a method of conservatively deciding which
backtracking points must be added for soundness, but not
so conservatively that all practical benefits are lost.

One such method is to use a \emph{visible operation dependency
graph}, and its use was proposed by \textbf{names} in an
algorithm which I will refer to as Stateful Dynamic Partial-
Order Reduction (SDPOR). The idea is that, when we first explore
a section of the state space, we keep track of the visible operations
that may be executed, so that if we encounter that section again,
we can cut short the search, and use our knowledge of the
visible operations that may be executed if the search was
to continue to add backtracking points as necessary.

In particular, a \emph{visible operation dependency
graph} is a graph $G = \langle V, E \rangle$ whose
nodes represent visible operations, and whose edges
represent the ordering of those transitions; whenever
a transition sequence of the form
$\sigma_1 \xrightarrow{t} \sigma_2 \xrightarrow{t'}
 \sigma_3$
is encountered in a search of the state space, the
edge $(t, t')$ is added to the visible operation
dependency graph $G$. Then, when a state $\sigma$ is
encountered which has already been explored on the
search, the set
$\{u \mid \exists t \in \textit{enabled}(\sigma).\;
   u \text{ is reachable from node } t \text{ in graph } G\}$
can be used to conservatively add backtracking points.

\section{Sleep Sets}
\textbf{Implementation of sleep
	sets in both the simple checker
	and in DPOR.}
\subsection{Simple Model Checker}
\subsection{DPOR}

\section{Counter-Example Traces} \label{sec:traces}

One of the advantages of model checking as
a formal method is that, in the case that
an error is found, it is trivial to give
an example of an execution of the given
program which leads to an error state; in
our case, this is just the sequence of
transitions executed at the point when
an error state it found.

To give more insight into the reason for the
error state being entered, it would be preferable
to instead present the use with a
\emph{partial order} of the transitions which,
whenever any linearisation of the partial order
is executed, results in the error/deadlock state.
Given a sequence of transitions which lead to
some error state, the problem is
then to give the most general partial order
which, however linearised, results a sequence
of transitions 

This partial order is just the happens-before
relation (cf. \ref{sec:happens-before}). As
discussed above  (\textbf{where?}), this can
be decided using clock vectors.
Just execute the transition sequence
from the beginning, keeping track of the relation.
Then output it in a nice form, so we can display it
as a graph maybe.

\begin{figure}	
	\lstinputlisting[firstline=39,lastline=57]{../src/program.ml}
	\caption{The function giving the partial order from
		a given transition sequence.}
\end{figure}

\chapter{Evaluation}

This chapter discusses the soundness
of the implementation of the model checking
algorithms, and analyses the performances
of both the model checking algorithms and
my implementations of them.

\section{Soundness}

\begin{center}
	\textit{But who will guard the guards themselves?} \\
	\qquad\qquad\qquad --- Juvenal, \textit{Satire VI}
\end{center}

Even though the algorithms
used in this project have all be proved sound \textbf{REFS},
there is no guarantee that my implementation
is therefore also sound, just because it
\emph{claims} to implement them; the isomorphism
between mathematical concepts in a research paper
and the manipulation of bits by a computer
is complex enough that bugs in the
implementation are more than just possible.
Even if another system was used to formally verify
the project, the obligation to trust
a software program would still remain,
not to mention the compiler, operating
system and hardware that any execution
of the program must use to run.

Given that it is all but impossible to
be fully convinced of its soundness,
what use is a model checking
system? If it might be unsound, then surely
none of its results can be trusted,
rendering it useless?

\subsection{Applications of Model Checking}
A model checking system does not have to be
formally proven to be sound to be of practical use.

\subsubsection{Proof as a Social Process}
\begin{itemize}
	\item Formal proofs aren't that great
	\item Proof is a social process whereby
	people become convinced of something
	\item In this case, convinced of its
	soundness by use over time and by
	examining source code and its
	relationship to published algorithms
	\item Particularly useful is using
	multiple systems to verify same code;
	it can be argued that if they agree
	then it is more likely that they are
	all correct than that they share the
	same implementation bugs.
\end{itemize}
\subsubsection{Unsound Systems Are Useful}
\begin{itemize}
	\item Even if a system isn't fully sound,
	it can certainly still be of practical use
	\item Errors localised, so can just consider
	a system which forbids use of that language
	feature
	\item Actually other aspects like efficiency
	and usability more important: a partially-sound
	system is still useful, but an unusable or
	really inefficient system is not
	\item Can still provide traces, which are
	the main advantage of model checking
\end{itemize}
\subsubsection{Model Checking as Rigorous Testing}
Advantages clear if thought of as rigorous
testing
\begin{itemize}
	\item Code coverage
	\item Fully automatic, not ad hoc
	\item Concurrency bugs
	\item Efficient (POR)
	\item Very useful traces; minimise detective work
\end{itemize}

\begin{center}
	*******
\end{center}

Although a model checking  program does not present
the user with any proof its claims when
it says ``I have checked the program you
gave me and it has no errors or deadlocks",
if it instead finds an error or deadlock state
in the system, its ability to present the
user with an example execution leading
to that state (cf. section~\ref{sec:traces})
is immensely useful in practice. As well
as identifying a previously-undiscovered
bug, showing the circumstances in which
the bug can arise gives an insight into
the bug which immediately puts the
developer on the track to
understanding and fixing it.

Therefore, even a model checking system that
is known to be unsound can be very worthwhile.
Suppose that there was a bug in my implementation
of recursive functions in PL -- the system can
still be fully utilised on any program (or
subprogram) which does not use recursion.
Thought of as an almost-watertight barrage
of ordinary tests, the power of automatic
model checking shines. With no human input
in the model checking process (unlike some
other formal methods, such as theorem
proving), the system checks the behaviour
of its inputs with full ``code coverage",
using techniques such as partial-order
reduction to do so as efficiently as possible.

While conventional testing provides limited
code coverage, model checking checks every
possible system behaviour; while 
many \textit{ad hoc} tests must
be handwritten for each program that
needs testing, model checking is fully automatic
and general;
while much detective work is often required to find
the bug following failure of a unit test,
model checking provides a clue leading straight to
the bug; and while concurrency bugs and race
conditions are notoriously difficult to find
using conventional tests, techniques such
as partial-order reduction allow model
checking of such systems with reasonable
efficiency.

\subsection{Evidence for Soundness}
Nevertheless, the more evidence that
can be given for the soundness of an
implementation, the more confidence
can be taken in its soundness, meaning
it is more likely to accept only
error-free and deadlock-free programs,
and find an error or deadlock state
if one exists. This section gives
some evidence that my implementations
of the algorithms are sound.

\subsubsection{Tests}
As part of the development process,
I wrote over forty test cases, each 
consisting of a transition system
represented as a PL program, and
the expected
$(\textit{error-free},\, \textit{deadlock-free})$
result of model checking that program.
Running an implementation of a model
checking algorithm on the program should
give the expected result: if not, then
either the result I expected was wrong
(usually due to an incorrect encoding
of the transition system in PL),
or the implementation was incorrect.
To ensure the former was not a possibility,
I used the Simple model checker, whose
correctness I have confidence in due
to its simplicity, to check that the
result I expected was in fact the
result that a correct implementation
should give.

\subsubsection{Examination of Source Code}
Programming in a high-level language such as
OCaml allowed me to keep the source code
relatively simple, and write it in such
a way that the correspondence between
the published pseudocode for an algorithm
and my source code implementing it was
as simple as possible, so that it
is easier to convince yourself that
some part of my source code correctly
implements the published algorithm.

\section{Performance}
\textbf{This section compares the
	performance of the various algorithms
	in the project. This is achieved by
	looking at execution times, transitions
	explored, and numbers of states explored
	while model checking various example
	programs. Comparison can be made with
	published figures.}

\begin{itemize}
	\item The performance depends strongly
	on the program being checked, so that
	analysing is difficult: there is no
	obvious ``average case"
	\item We will instead use several
	benchmarks published in the literature
	for analysis of model checking performance
	\item As well as comparing the various
	algorithms to each other, I will also
	compare with published results
	\item Transitions are one measure of
	performance: the reduction in the size
	of the state space is a property of the
	algorithm, not the implementation, 
	assuming the implementation is correct
	\item (In fact transitions can therefore
	be used as evidence for correctness)
	\item Execution time is another measure
	of performance: the relationship between
	execution time and transitions explored
	gives information about both the
	intrinsic complexity of the algorithm
	and specifically the efficiency of my
	implementation. Separating these two
	aspects will be done below.
	\item Analysis will be done of the above
	things as a function of number of threads,
	and amount of computation in each thread.
\end{itemize}

\subsection{Reduction of State Space Searched}
\subsubsection{Comparison with Each Other}
Here, we will analyse the reduction in the size of the
state space that each model checking algorithm
offers in comparison to the simple model checker.
\subsubsection{Comparison with Published Results}

\subsection{Execution Times of Model Checking}
\subsubsection{Comparison with Each Other}
\subsubsection{Comparison with Published Results}

\chapter{Conclusions}

\textbf{This chapter makes some
	interesting conclusions, which I
	haven't thought of yet.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{PL Definition}
 
\textbf{This will be the definition
	of PL.}
 
\chapter{Example PL Programs}

\textbf{This will contain various
	example programs, in particular
	those used for performance evaluation.}

\chapter{Project Proposal}

Project proposal here
% \input{proposal_without_title}

\end{document}
